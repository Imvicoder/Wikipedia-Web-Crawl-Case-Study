{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import time\n",
    "def find_url(url):\n",
    "    srccode=urllib.request.urlopen(url).read()\n",
    "    bsobj=bs.BeautifulSoup(srccode,'lxml')\n",
    "    url=[]\n",
    "    divs=bsobj.find('div',id='mw-content-text').find('div',class_='mw-parser-output')\n",
    "    for i in (divs.find_all('p',recursive=False)):\n",
    "        if i.find('a',recursive=False):\n",
    "            url=i.find('a',recursive=False).get('href')\n",
    "            break\n",
    "    first_url=urllib.parse.urljoin('https://en.wikipedia.org/', url)        \n",
    "    return first_url    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url=find_url('https://en.wikipedia.org/wiki/PICALM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Protein\n"
     ]
    }
   ],
   "source": [
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawl(crawled_urls,target_url,max_visited_url=25):\n",
    "    if crawled_urls[-1]==target_url:\n",
    "        print(\"We have arrived at target url..!!\")\n",
    "        return False\n",
    "    elif len(crawled_urls)>max_visited_url:\n",
    "        print(\"we have hachived maximum permissible crawling limits,So need to abort this search\")\n",
    "        return False\n",
    "    elif crawled_urls[-1] in crawled_urls[:-1]:\n",
    "        print(\"This URL already visited!! We might be studk in loop/cycle\")\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/South_African_cliff_swallow\n",
      "https://en.wikipedia.org/wiki/Bird\n",
      "https://en.wikipedia.org/wiki/Endotherm\n"
     ]
    }
   ],
   "source": [
    "url_list=['https://en.wikipedia.org/wiki/South_African_cliff_swallow']\n",
    "target_url='https://en.wikipedia.org/wiki/Vernacular'\n",
    "while crawl(url_list, target_url):\n",
    "    print(url_list[-1])\n",
    "\n",
    "    first_link = find_url(url_list[-1])\n",
    "    if not first_link:\n",
    "        print(\"We've arrived at an article with no links, aborting search!\")\n",
    "        break\n",
    "\n",
    "    url_list.append(first_link)\n",
    "\n",
    "    time.sleep(2) # Slow things down so as to not hammer Wikipedia's servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
